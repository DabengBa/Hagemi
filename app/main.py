from fastapi import FastAPI, HTTPException, Request, Depends, status, Response
from fastapi.responses import JSONResponse, StreamingResponse, HTMLResponse
from .models import ChatCompletionRequest, ChatCompletionResponse, ErrorResponse, ModelList, Message, ToolCall, ChoiceDelta
from .gemini import GeminiClient, ResponseWrapper
from .utils import handle_gemini_error, protect_from_abuse, APIKeyManager, test_api_key
import os
import json
import asyncio
from typing import Literal
import random
import requests
from datetime import datetime, timedelta, timezone
from apscheduler.schedulers.background import BackgroundScheduler
import sys
import logging

logging.getLogger("uvicorn").disabled = True
logging.getLogger("uvicorn.access").disabled = True

# é…ç½® logger
logger = logging.getLogger("my_logger")
logger.setLevel(logging.DEBUG)

def translate_error(message: str) -> str:
    if "quota exceeded" in message.lower():
        return "API å¯†é’¥é…é¢å·²ç”¨å°½"
    if "invalid argument" in message.lower():
        return "æ— æ•ˆå‚æ•°"
    if "internal server error" in message.lower():
        return "æœåŠ¡å™¨å†…éƒ¨é”™è¯¯"
    if "service unavailable" in message.lower():
        return "æœåŠ¡ä¸å¯ç”¨"
    return message


def handle_exception(exc_type, exc_value, exc_traceback):
    if issubclass(exc_type, KeyboardInterrupt):
        sys.excepthook(exc_type, exc_value, exc_traceback)
        return
    error_message = translate_error(str(exc_value))
    logger.error(f"æœªæ•è·çš„å¼‚å¸¸: %s" % error_message, extra={'status_code': 500, 'error_message': error_message, 'key': 'N/A', 'request_type': 'N/A', 'model': 'N/A'})


sys.excepthook = handle_exception

app = FastAPI()

PASSWORD = os.environ.get("PASSWORD", "123")
SECOND_MODEL = os.environ.get("SECOND_MODEL", "gemini-2.5-flash-preview-04-17")
MAX_RETRY = int(os.environ.get("MAX_RETRY", "3")) 
MAX_REQUESTS_PER_MINUTE = int(os.environ.get("MAX_REQUESTS_PER_MINUTE", "4"))
MAX_REQUESTS_PER_DAY_PER_IP = int(
    os.environ.get("MAX_REQUESTS_PER_DAY_PER_IP", "200"))
RETRY_DELAY = 3
safety_settings = [
    {
        "category": "HARM_CATEGORY_HARASSMENT",
        "threshold": "BLOCK_NONE"
    },
    {
        "category": "HARM_CATEGORY_HATE_SPEECH",
        "threshold": "BLOCK_NONE"
    },
    {
        "category": "HARM_CATEGORY_SEXUALLY_EXPLICIT",
        "threshold": "BLOCK_NONE"
    },
    {
        "category": "HARM_CATEGORY_DANGEROUS_CONTENT",
        "threshold": "BLOCK_NONE"
    },
    {
        "category": 'HARM_CATEGORY_CIVIC_INTEGRITY',
        "threshold": 'BLOCK_NONE'
    }
]
safety_settings_g2 = [
    {
        "category": "HARM_CATEGORY_HARASSMENT",
        "threshold": "OFF"
    },
    {
        "category": "HARM_CATEGORY_HATE_SPEECH",
        "threshold": "OFF"
    },
    {
        "category": "HARM_CATEGORY_SEXUALLY_EXPLICIT",
        "threshold": "OFF"
    },
    {
        "category": "HARM_CATEGORY_DANGEROUS_CONTENT",
        "threshold": "OFF"
    },
    {
        "category": 'HARM_CATEGORY_CIVIC_INTEGRITY',
        "threshold": 'OFF'
    }
]
GOOGLE_SEARCH_MODELS = {
    "gemini-2.0-flash-exp",
    "gemini-2.0-flash",
    "gemini-2.5-pro-preview-03-25",
    "gemini-2.5-pro-exp-03-25",
}

THINKING_BUDGET_MODELS = {
    "gemini-2.5-flash-preview-04-17",
}

<<<<<<< HEAD
<<<<<<< HEAD
logger.info("å³å°†å®ä¾‹åŒ– APIKeyManager", extra={'key': 'N/A', 'request_type': 'N/A', 'model': 'N/A', 'status_code': 'N/A', 'error_message': ''})
key_manager = APIKeyManager() # å®ä¾‹åŒ– APIKeyManagerï¼Œæ ˆä¼šåœ¨ __init__ ä¸­åˆå§‹åŒ–
logger.info("APIKeyManager å®ä¾‹åŒ–å®Œæˆ", extra={'key': 'N/A', 'request_type': 'N/A', 'model': 'N/A', 'status_code': 'N/A', 'error_message': ''})
current_api_key = key_manager.get_available_key()
logger.info("è·å–å¯ç”¨å¯†é’¥å®Œæˆ", extra={'key': 'N/A', 'request_type': 'N/A', 'model': 'N/A', 'status_code': 'N/A', 'error_message': ''})
=======
key_manager = APIKeyManager() # å®ä¾‹åŒ– APIKeyManagerï¼Œæ ˆä¼šåœ¨ __init__ ä¸­åˆå§‹åŒ–
current_api_key = key_manager.get_available_key()
>>>>>>> parent of f44bd50 (æ›´æ–°thinkingæ¨¡å‹list)
=======
key_manager = APIKeyManager() # å®ä¾‹åŒ– APIKeyManagerï¼Œæ ˆä¼šåœ¨ __init__ ä¸­åˆå§‹åŒ–
current_api_key = key_manager.get_available_key()
>>>>>>> parent of f44bd50 (æ›´æ–°thinkingæ¨¡å‹list)


def switch_api_key():
    global current_api_key
    key = key_manager.get_available_key() # get_available_key ä¼šå¤„ç†æ ˆçš„é€»è¾‘
    if key:
        current_api_key = key
        logger.info(f"API key æ›¿æ¢ä¸º â†’ {current_api_key[-6:]}...", extra={'key': current_api_key[-6:], 'request_type': 'switch_key', 'model': 'N/A', 'status_code': 'N/A', 'error_message': ''})
    else:
        logger.error("API key æ›¿æ¢å¤±è´¥ï¼Œæ‰€æœ‰API keyéƒ½å·²å°è¯•ï¼Œè¯·é‡æ–°é…ç½®æˆ–ç¨åé‡è¯•", extra={'key': 'N/A', 'request_type': 'switch_key', 'model': 'N/A', 'status_code': 'N/A', 'error_message': ''})


async def check_keys():
    available_keys = []
    for key in key_manager.api_keys:
        is_valid = await test_api_key(key)
        status_msg = "æœ‰æ•ˆ" if is_valid else "æ— æ•ˆ"
        logger.info(f"API Key {key[:10]}... {status_msg}.", extra={'key': key[-6:], 'request_type': 'startup', 'model': 'N/A', 'status_code': 'N/A', 'error_message': ''})
        if is_valid:
            available_keys.append(key)
    if not available_keys:
        logger.error("æ²¡æœ‰å¯ç”¨çš„ API å¯†é’¥ï¼", extra={'key': 'N/A', 'request_type': 'startup', 'model': 'N/A', 'status_code': 'N/A', 'error_message': ''})
    return available_keys


@app.on_event("startup")
async def startup_event():
    logger.info("Starting Gemini API proxy...", extra={'key': 'N/A', 'request_type': 'startup', 'model': 'N/A', 'status_code': 'N/A', 'error_message': ''})
    available_keys = await check_keys()
    if available_keys:
        key_manager.api_keys = available_keys
        key_manager._reset_key_stack() # å¯åŠ¨æ—¶ä¹Ÿç¡®ä¿åˆ›å»ºéšæœºæ ˆ
        key_manager.show_all_keys()
        logger.info(f"å¯ç”¨ API å¯†é’¥æ•°é‡ï¼š{len(key_manager.api_keys)}", extra={'key': 'N/A', 'request_type': 'startup', 'model': 'N/A', 'status_code': 'N/A', 'error_message': ''})
        # MAX_RETRIES = len(key_manager.api_keys)
        retry_attempts = min(MAX_RETRY, len(key_manager.api_keys)) if key_manager.api_keys else 1
        logger.info(f"æœ€å¤§é‡è¯•æ¬¡æ•°è®¾ç½®ä¸ºï¼š{retry_attempts}", extra={'key': 'N/A', 'request_type': 'startup', 'model': 'N/A', 'status_code': 'N/A', 'error_message': ''}) # æ·»åŠ æ—¥å¿—
        if key_manager.api_keys:
            all_models = await GeminiClient.list_available_models(key_manager.api_keys[0])
            GeminiClient.AVAILABLE_MODELS = all_models
            logger.info(f"Available models loaded: {len(all_models)} models", extra={'key': 'N/A', 'request_type': 'startup', 'model': 'N/A', 'status_code': 'N/A', 'error_message': ''})

@app.get("/v1/models", response_model=ModelList)
def list_models():
    logger.info("Received request to list models", extra={'request_type': 'list_models', 'status_code': 200, 'key': 'N/A', 'model': 'N/A', 'error_message': ''})
    return ModelList(data=[{"id": model, "object": "model", "created": 1678888888, "owned_by": "organization-owner"} for model in GeminiClient.AVAILABLE_MODELS])


async def verify_password(request: Request):
    if PASSWORD:
        auth_header = request.headers.get("Authorization")
        if not auth_header or not auth_header.startswith("Bearer "):
            raise HTTPException(
                status_code=401, detail="Unauthorized: Missing or invalid token")
        token = auth_header.split(" ")[1]
        if token != PASSWORD:
            raise HTTPException(
                status_code=401, detail="Unauthorized: Invalid token")


async def process_request(chat_request: ChatCompletionRequest, http_request: Request, request_type: Literal['stream', 'non-stream']):
    global current_api_key
    protect_from_abuse(
        http_request, MAX_REQUESTS_PER_MINUTE, MAX_REQUESTS_PER_DAY_PER_IP)

    key_manager.reset_tried_keys_for_request() # åœ¨æ¯æ¬¡è¯·æ±‚å¤„ç†å¼€å§‹æ—¶é‡ç½® tried_keys é›†åˆ

    try:
        contents, system_instruction = GeminiClient.convert_messages(
            GeminiClient, chat_request.messages)
    except ValueError as e:
         logger.error(f"æ¶ˆæ¯è½¬æ¢å¤±è´¥: {e}", extra={'key': 'N/A', 'request_type': request_type, 'model': chat_request.model, 'status_code': 400, 'error_message': str(e)})
         raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail=f"Failed to convert messages: {e}")

    retry_attempts = min(MAX_RETRY, len(key_manager.api_keys)) if key_manager.api_keys else 1 # é‡è¯•æ¬¡æ•°ç­‰äºå¯†é’¥æ•°é‡å’ŒMAX_RETRYä¹‹ä¸­æœ€å°å€¼ï¼Œè‡³å°‘å°è¯• 1 æ¬¡
<<<<<<< HEAD
<<<<<<< HEAD
    api_version_to_use = "v1beta" # Start with v1beta as it's generally more stable for tools
    use_thinking_budget = chat_request.model in THINKING_BUDGET_MODELS
    tools = chat_request.tools
    tool_choice = chat_request.tool_choice
=======
    api_version_to_use = "v1alpha" # Start with v1alpha
>>>>>>> parent of f44bd50 (æ›´æ–°thinkingæ¨¡å‹list)
=======
    api_version_to_use = "v1alpha" # Start with v1alpha
>>>>>>> parent of f44bd50 (æ›´æ–°thinkingæ¨¡å‹list)

    for attempt in range(1, retry_attempts + 1):
        # Get a key only if it's the first attempt or after a key switch (not after version switch)
        if attempt == 1 or 'switch_key_occurred' in locals() and switch_key_occurred:
             current_api_key = key_manager.get_available_key()
             api_version_to_use = "v1alpha" # Reset to v1alpha when getting a new key
             switch_key_occurred = False # Reset flag

        if current_api_key is None: # æ£€æŸ¥æ˜¯å¦è·å–åˆ° API å¯†é’¥
            logger.warning("æ²¡æœ‰å¯ç”¨çš„ API å¯†é’¥ï¼Œè·³è¿‡æœ¬æ¬¡å°è¯•", extra={'request_type': request_type, 'model': chat_request.model, 'status_code': 'N/A', 'key': 'N/A', 'error_message': ''})
            break  # å¦‚æœæ²¡æœ‰å¯ç”¨å¯†é’¥ï¼Œè·³å‡ºå¾ªç¯

        extra_log = {'key': current_api_key[-6:], 'request_type': request_type, 'model': chat_request.model, 'status_code': 'N/A', 'error_message': '', 'api_version': api_version_to_use}
        logger.info(f"ç¬¬ {attempt}/{retry_attempts} æ¬¡å°è¯• (API Version: {api_version_to_use})", extra=extra_log)

        gemini_client = GeminiClient(current_api_key)
        try:
            if chat_request.stream:
                async def stream_generator():
                    chunk_id_counter = 0
                    try:
<<<<<<< HEAD
<<<<<<< HEAD
                        async for chunk_data in gemini_client.stream_chat(
                            chat_request, contents,
                            safety_settings_g2 if 'gemini-2.0-flash-exp' in chat_request.model else safety_settings,
                            system_instruction,
                            tools=tools, # Pass tools
                            tool_choice=tool_choice, # Pass tool_choice
                            api_version=api_version_to_use,
                            use_thinking_budget=use_thinking_budget
                        ):
                            chunk_id_counter += 1
                            chunk_id = f"chatcmpl-chunk-{chunk_id_counter}"
                            created_time = int(time.time())

                            if chunk_data["type"] == "delta":
                                delta_content = chunk_data["content"]
                                # Map Gemini delta to OpenAI delta format
                                choice_delta = ChoiceDelta(**delta_content) # Directly use if structure matches
                                choice = Choice(index=0, delta=choice_delta, finish_reason=None, message=None) # message is not needed for delta

                            elif chunk_data["type"] == "final":
                                finish_reason = chunk_data["finish_reason"]
                                choice = Choice(index=0, delta=ChoiceDelta(), finish_reason=finish_reason, message=None) # Empty delta for final chunk

                            else:
                                logger.warning(f"Unknown chunk type received: {chunk_data.get('type')}")
                                continue

                            # Create OpenAI compatible chunk response
                            formatted_chunk = ChatCompletionResponse(
                                id=chunk_id,
                                object="chat.completion.chunk",
                                created=created_time,
                                model=chat_request.model,
                                choices=[choice],
                                usage=None # Usage is typically not in chunks
                            )
                            yield f"data: {formatted_chunk.model_dump_json(exclude_unset=True)}\n\n"

                        # Send DONE message after the loop finishes successfully
=======
=======
>>>>>>> parent of f44bd50 (æ›´æ–°thinkingæ¨¡å‹list)
                        async for chunk in gemini_client.stream_chat(chat_request, contents, safety_settings_g2 if 'gemini-2.0-flash-exp' in chat_request.model else safety_settings, system_instruction, api_version=api_version_to_use):
                            formatted_chunk = {"id": "chatcmpl-someid", "object": "chat.completion.chunk", "created": 1234567,
                                               "model": chat_request.model, "choices": [{"delta": {"role": "assistant", "content": chunk}, "index": 0, "finish_reason": None}]}
                            yield f"data: {json.dumps(formatted_chunk)}\n\n"
>>>>>>> parent of f44bd50 (æ›´æ–°thinkingæ¨¡å‹list)
                        yield "data: [DONE]\n\n"

                    except asyncio.CancelledError:
                        extra_log_cancel = {'key': current_api_key[-6:], 'request_type': request_type, 'model': chat_request.model, 'error_message': 'å®¢æˆ·ç«¯å·²æ–­å¼€è¿æ¥'}
                        logger.info("å®¢æˆ·ç«¯è¿æ¥å·²ä¸­æ–­", extra=extra_log_cancel)
                    except Exception as e:
                        # Handle error within the stream generator needs careful thought,
                        # For now, let the outer exception handler catch it to trigger retry/version switch
                        raise e
                        # error_detail = handle_gemini_error(
                        #     e, current_api_key, key_manager)
                        # yield f"data: {json.dumps({'error': {'message': error_detail, 'type': 'gemini_error'}})}\n\n"
                return StreamingResponse(stream_generator(), media_type="text/event-stream")
            else:
                async def run_gemini_completion():
                    try:
<<<<<<< HEAD
<<<<<<< HEAD
                        # Pass tools and tool_choice to complete_chat
                        response_wrapper: ResponseWrapper = await asyncio.to_thread(
                            gemini_client.complete_chat,
                            chat_request, contents,
                            safety_settings_g2 if 'gemini-2.0-flash-exp' in chat_request.model else safety_settings,
                            system_instruction,
                            tools=tools, # Pass tools
                            tool_choice=tool_choice, # Pass tool_choice
                            api_version=api_version_to_use,
                            use_thinking_budget=use_thinking_budget
                        )
                        return response_wrapper
=======
=======
>>>>>>> parent of f44bd50 (æ›´æ–°thinkingæ¨¡å‹list)
                        response_content = await asyncio.to_thread(gemini_client.complete_chat, chat_request, contents, safety_settings_g2 if 'gemini-2.0-flash-exp' in chat_request.model else safety_settings, system_instruction, api_version=api_version_to_use)
                        return response_content
>>>>>>> parent of f44bd50 (æ›´æ–°thinkingæ¨¡å‹list)
                    except asyncio.CancelledError:
                        extra_log_gemini_cancel = {'key': current_api_key[-6:], 'request_type': request_type, 'model': chat_request.model, 'error_message': 'å®¢æˆ·ç«¯æ–­å¼€å¯¼è‡´APIè°ƒç”¨å–æ¶ˆ', 'api_version': api_version_to_use}
                        logger.info("APIè°ƒç”¨å› å®¢æˆ·ç«¯æ–­å¼€è€Œå–æ¶ˆ", extra=extra_log_gemini_cancel)
                        raise

                gemini_task = asyncio.create_task(run_gemini_completion())

                try:
                    done, pending = await asyncio.wait(
                        [gemini_task],
                        return_when=asyncio.FIRST_COMPLETED
                    )

                    if gemini_task in done:
                        response_wrapper = gemini_task.result() # Now it's a ResponseWrapper

                        # Check for empty response or only thoughts
                        if not response_wrapper.text and not response_wrapper.tool_calls:
                            extra_log_empty_response = {'key': current_api_key[-6:], 'request_type': request_type, 'model': chat_request.model, 'status_code': 204, 'finish_reason': response_wrapper.finish_reason}
                            logger.info(f"Gemini API è¿”å›ç©ºå“åº”æˆ–ä»…åŒ…å«æ€è€ƒè¿‡ç¨‹ (Finish Reason: {response_wrapper.finish_reason})", extra=extra_log_empty_response)

                            # Handle empty response retry logic (existing logic seems okay)
                            if response_wrapper.finish_reason != 'STOP' and response_wrapper.finish_reason != 'tool_calls': # Don't retry if it stopped normally or for tool calls
                                if chat_request.model != SECOND_MODEL:
                                    logger.info(f"å°è¯•åˆ‡æ¢åˆ°å¤‡ç”¨æ¨¡å‹ {SECOND_MODEL}", extra={'key': current_api_key[-6:], 'request_type': request_type, 'model': SECOND_MODEL, 'status_code': 'N/A', 'error_message': ''})
                                    chat_request.model = SECOND_MODEL
                                    continue # Retry with the second model

                            # If still empty or stopped normally/tool_calls, return an empty message or handle as needed
                            # For now, let's return an empty assistant message if no text/tools
                            assistant_message = Message(role="assistant", content="")
                            finish_reason = response_wrapper.finish_reason if response_wrapper.finish_reason else "stop"
                            response = ChatCompletionResponse(
                                id="chatcmpl-empty", object="chat.completion", created=int(time.time()), model=chat_request.model,
                                choices=[Choice(index=0, message=assistant_message, finish_reason=finish_reason)]
                                # Usage might be available even for empty responses
                                # usage=Usage(prompt_tokens=response_wrapper.prompt_token_count or 0, completion_tokens=response_wrapper.candidates_token_count or 0, total_tokens=response_wrapper.total_token_count or 0)
                            )
                            # Log success even for empty response if finish reason is STOP/tool_calls
                            extra_log_success = {'key': current_api_key[-6:], 'request_type': request_type, 'model': chat_request.model, 'status_code': 200, 'error_message': '', 'finish_reason': finish_reason}
                            logger.info("è¯·æ±‚å¤„ç†æˆåŠŸ (ç©ºå“åº”)", extra=extra_log_success)
                            # Fall through to return the empty response

                        else:
                            # Create assistant message with text and/or tool calls
                            assistant_content = response_wrapper.text
                            tool_calls_list = None
                            if response_wrapper.tool_calls:
                                tool_calls_list = [ToolCall(**tc) for tc in response_wrapper.tool_calls] # Validate with Pydantic

                            assistant_message = Message(
                                role="assistant",
                                content=assistant_content if assistant_content else None, # Content is None if only tool calls
                                tool_calls=tool_calls_list
                            )

                            # Determine finish reason
                            finish_reason = response_wrapper.finish_reason
                            if finish_reason == "TOOL_CALLS":
                                finish_reason = "tool_calls" # Map to OpenAI's reason
                            elif not finish_reason or finish_reason == "STOP":
                                finish_reason = "stop"
                            elif finish_reason == "MAX_TOKENS":
                                finish_reason = "length"
                            # Add other mappings if necessary (e.g., SAFETY -> stop)

                            response = ChatCompletionResponse(
                                id="chatcmpl-someid", object="chat.completion", created=int(time.time()), model=chat_request.model,
                                choices=[Choice(index=0, message=assistant_message, finish_reason=finish_reason)],
                                usage=Usage(prompt_tokens=response_wrapper.prompt_token_count or 0, completion_tokens=response_wrapper.candidates_token_count or 0, total_tokens=response_wrapper.total_token_count or 0)
                            )
                            extra_log_success = {'key': current_api_key[-6:], 'request_type': request_type, 'model': chat_request.model, 'status_code': 200, 'error_message': '', 'finish_reason': finish_reason}
                            logger.info("è¯·æ±‚å¤„ç†æˆåŠŸ", extra=extra_log_success)

                        # --- Existing disconnection check and return logic ---
                        try:
                            is_disconnected = await http_request.is_disconnected()
                        except Exception as e_disconnect:
                            # è®°å½• is_disconnected è°ƒç”¨æœ¬èº«çš„é”™è¯¯
                            extra_log_disconnect_error = {'key': current_api_key[-6:] if current_api_key else 'N/A', 'request_type': request_type, 'model': chat_request.model, 'error_message': f"Error calling http_request.is_disconnected: {e_disconnect}"}
                            logger.error("è°ƒç”¨ http_request.is_disconnected æ—¶å‡ºé”™", extra=extra_log_disconnect_error)
                            # å‡è®¾æœªæ–­å¼€ï¼Œè®©åç»­é€»è¾‘ç»§ç»­å°è¯•è¿”å›æ•°æ®
                            is_disconnected = False

                        if is_disconnected:
                            extra_log_client_disconnect = {'key': current_api_key[-6:] if current_api_key else 'N/A', 'request_type': request_type, 'model': chat_request.model, 'error_message': 'æ£€æµ‹åˆ°å®¢æˆ·ç«¯æ–­å¼€è¿æ¥'}
                            logger.info("å®¢æˆ·ç«¯è¿æ¥å·²ä¸­æ–­ (ä½†ä»å°è¯•è¿”å›)", extra=extra_log_client_disconnect)
                            if response_content.text:
                                extra_log_response = {
                                    'key': current_api_key[-6:] if current_api_key else 'N/A',
                                    'request_type': request_type,
                                    'model': chat_request.model,
                                    'response_content': response_content.text
                                }
                                logger.info(f"è·å–åˆ°å“åº”å†…å®¹ (å®¢æˆ·ç«¯å·²æ–­å¼€): {extra_log_response['response_content']}", extra=extra_log_response)
                            # æ³¨æ„ï¼šå³ä½¿å®¢æˆ·ç«¯æ–­å¼€ï¼Œæˆ‘ä»¬ä»ç„¶å°è¯•è¿”å›å“åº”
                            # å¦‚æœè¿”å›å¤±è´¥ï¼ŒFastAPI/Uvicorn ä¼šå¤„ç†åº•å±‚çš„è¿æ¥é”™è¯¯
                        try:
                            return response # æ— è®º is_disconnected æ£€æŸ¥æ˜¯å¦å‡ºé”™æˆ–ç»“æœå¦‚ä½•ï¼Œéƒ½å°è¯•è¿”å›
                        except Exception as e_return:
                            # è®°å½•åœ¨è¿”å›å“åº”æ—¶å‘ç”Ÿçš„é”™è¯¯
                            extra_log_return_error = {'key': current_api_key[-6:] if current_api_key else 'N/A', 'request_type': request_type, 'model': chat_request.model, 'error_message': f"Error during return response: {e_return}"}
                            logger.error("è¿”å›å“åº”æ—¶å‘ç”Ÿé”™è¯¯", extra=extra_log_return_error)
                            # é‡æ–°æŠ›å‡ºå¼‚å¸¸ï¼Œè®© FastAPI æˆ– ASGI æœåŠ¡å™¨å¤„ç†ï¼Œé¿å…è§¦å‘å¤–å±‚é‡è¯•
                            raise e_return

                except asyncio.CancelledError:
                    extra_log_request_cancel = {'key': current_api_key[-6:] if current_api_key else 'N/A', 'request_type': request_type, 'model': chat_request.model, 'error_message':"è¯·æ±‚è¢«å–æ¶ˆ" }
                    logger.info("è¯·æ±‚å–æ¶ˆ", extra=extra_log_request_cancel)
                    raise

        except HTTPException as e:
            if e.status_code == status.HTTP_408_REQUEST_TIMEOUT:
                extra_log = {'key': current_api_key[-6:], 'request_type': request_type, 'model': chat_request.model,
                            'status_code': 408, 'error_message': 'å®¢æˆ·ç«¯è¿æ¥ä¸­æ–­'}
                logger.error("å®¢æˆ·ç«¯è¿æ¥ä¸­æ–­ï¼Œç»ˆæ­¢åç»­é‡è¯•", extra=extra_log)
                raise  
            else:
                raise  
        except Exception as e:
            error_type = handle_gemini_error(e, current_api_key, key_manager)
            
            # Check if it's a 500 error and we are using v1alpha
            # Adjust error handling for API version switching if needed
            # Since we start with v1beta now, maybe remove the v1alpha -> v1beta switch logic
            # Or adjust based on specific errors encountered with v1beta
            if error_type == "GEMINI_500_ERROR": # Handle 500 errors (potentially try switching key)
                logger.warning(f"é‡åˆ° 500 é”™è¯¯ (API Version: {api_version_to_use})", extra={'key': current_api_key[-6:], 'request_type': request_type, 'model': chat_request.model, 'status_code': 500, 'error_message': 'Gemini 500 Error'})
                # Fall through to standard retry logic (switch key)
            elif error_type == "æ— æ•ˆçš„ API å¯†é’¥": # Handle invalid key specifically
                 logger.error(f"API å¯†é’¥æ— æ•ˆ: ...{current_api_key[-6:]}", extra={'key': current_api_key[-6:], 'request_type': request_type, 'model': chat_request.model, 'status_code': 400, 'error_message': 'Invalid API Key'})
                 # Key is likely bad, proceed to switch key immediately in retry logic
            elif error_type == "API å¯†é’¥é…é¢å·²ç”¨å°½æˆ–å…¶ä»–åŸå› ": # Handle 429 specifically
                 logger.warning(f"API å¯†é’¥é™æµ: ...{current_api_key[-6:]}", extra={'key': current_api_key[-6:], 'request_type': request_type, 'model': chat_request.model, 'status_code': 429, 'error_message': 'Rate Limit/Quota Exceeded'})
                 # Key is rate-limited, proceed to switch key immediately in retry logic

            # Standard retry logic: switch key if attempts remain
            if attempt < retry_attempts:
                logger.info(f"å°è¯•åˆ‡æ¢ API Key (å½“å‰: ...{current_api_key[-6:]})", extra={'key': current_api_key[-6:], 'request_type': request_type, 'model': chat_request.model, 'status_code': 'N/A', 'error_message': f'Attempt {attempt} failed, switching key'})
                switch_api_key()
                switch_key_occurred = True # Flag that key was switched
                api_version_to_use = "v1beta" # Reset to v1beta when switching key
                await asyncio.sleep(RETRY_DELAY) # æ·»åŠ é‡è¯•å»¶è¿Ÿ
                continue # Continue to the next attempt in the loop
            else:
                # This is the final attempt, and it failed. Log it.
                logger.error(f"æœ€åä¸€æ¬¡å°è¯•å¤±è´¥ (Attempt {attempt}/{retry_attempts})ï¼Œé”™è¯¯: {error_type}", extra={'key': current_api_key[-6:] if current_api_key else 'N/A', 'request_type': request_type, 'model': chat_request.model, 'status_code': 'N/A', 'error_message': f'Final attempt failed: {error_type}'})
                # No 'continue' here. The loop will terminate after this block,
                # and the code will proceed to raise the final HTTPException outside the loop.
                pass # Explicitly pass to make the else block valid

    msg = "æ‰€æœ‰APIå¯†é’¥å‡å¤±è´¥,è¯·ç¨åé‡è¯•"
    extra_log_all_fail = {'key': "ALL", 'request_type': request_type, 'model': chat_request.model, 'status_code': 500, 'error_message': msg}
    logger.error(msg, extra=extra_log_all_fail)
    raise HTTPException(
        status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=msg)


# Allow response_model to be Union[ChatCompletionResponse, StreamingResponse] ? No, FastAPI handles StreamingResponse separately.
# The return type hint should reflect what's actually returned.
@app.post("/v1/chat/completions") # Remove response_model here as it varies
async def chat_completions(request: ChatCompletionRequest, http_request: Request, _: None = Depends(verify_password)) -> Response: # Return type is Response
    response = await process_request(request, http_request, "stream" if request.stream else "non-stream")
    return response


@app.exception_handler(Exception)
async def global_exception_handler(request: Request, exc: Exception):
    error_message = translate_error(str(exc))
    extra_log_unhandled_exception = {'status_code': 500, 'error_message': error_message}
    logger.error(f"Unhandled exception: {error_message}", extra=extra_log_unhandled_exception)
    return JSONResponse(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, content=ErrorResponse(message=str(exc), type="internal_error").dict())


@app.get("/", response_class=HTMLResponse)
async def root():
    html_content = f"""
    <!DOCTYPE html>
    <html>
    <head>
        <title>Gemini API ä»£ç†æœåŠ¡</title>
        <style>
            body {{
                font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
                max-width: 800px;
                margin: 0 auto;
                padding: 20px;
                line-height: 1.6;
            }}
            h1 {{
                color: #333;
                text-align: center;
                margin-bottom: 30px;
            }}
            .info-box {{
                background-color: #f8f9fa;
                border: 1px solid #dee2e6;
                border-radius: 4px;
                padding: 20px;
                margin-bottom: 20px;
            }}
            .status {{
                color: #28a745;
                font-weight: bold;
            }}
        </style>
    </head>
    <body>
        <h1>ğŸ¤– Gemini API ä»£ç†æœåŠ¡</h1>
        
        <div class="info-box">
            <h2>ğŸŸ¢ è¿è¡ŒçŠ¶æ€</h2>
            <p class="status">æœåŠ¡è¿è¡Œä¸­</p>
            <p>APIå¯†é’¥æ•°é‡: {len(key_manager.api_keys)}</p>
            <p>å¯ç”¨æ¨¡å‹æ•°é‡: {len(GeminiClient.AVAILABLE_MODELS)}</p>
            <div style='margin-top:15px;'>
                <h3>APIå¯†é’¥çŠ¶æ€ï¼š</h3>
                <ul style='list-style:none; padding-left:0;'>
                    {"".join([f'<li style="margin-bottom:8px;">ğŸ”‘ ...{key[-6:]} | æœ€è¿‘429é”™è¯¯: {(datetime.fromtimestamp(error_time).astimezone(timezone(timedelta(hours=8))).strftime("%Y-%m-%d %H:%M:%S") if (error_time := key_manager.key_error_times.get(key)) else "æ— é”™è¯¯è®°å½•")}</li>' for key in key_manager.api_keys])}
                </ul>
            </div>
        </div>

        <div class="info-box">
            <h2>âš™ï¸ ç¯å¢ƒé…ç½®</h2>
            <p>æ¯åˆ†é’Ÿè¯·æ±‚é™åˆ¶: {MAX_REQUESTS_PER_MINUTE}</p>
            <p>æ¯IPæ¯æ—¥è¯·æ±‚é™åˆ¶: {MAX_REQUESTS_PER_DAY_PER_IP}</p>
            <p>æœ€å¤§é‡è¯•æ¬¡æ•°: {min(MAX_RETRY, len(key_manager.api_keys)) if key_manager.api_keys else 1}</p>
        </div>
    </body>
    </html>
    """
    return html_content
